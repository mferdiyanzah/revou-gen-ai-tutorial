{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4fecfb",
   "metadata": {},
   "source": [
    "# LAB 2: Prompt Engineering\n",
    "\n",
    "Effective prompts give Security Copilot adequate and useful parameters to generate a valuable response. Security analysts or researchers should include the following elements when writing a prompt.\n",
    "\n",
    "Goal - specific, security-related information that you need\n",
    "\n",
    "Context - why you need this information or how you plan to use it\n",
    "\n",
    "Expectations - format or target audience you want the response tailored to\n",
    "\n",
    "Source - known information, data sources, or plugins Security Copilot should use\n",
    "\n",
    "<img src=\"./assets/prompt-elements.png\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce05650",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaca00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you have not installed langchain in your environment\n",
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading environment variables \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  # take environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Langchain Chat Models\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4.1-mini\", model_provider= \"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot prompting\n",
    "\n",
    "prompt = '''\n",
    "    You are a team of technical writers assigned to create a blog post exploring the intersection of technology and artificial intelligence. \n",
    "\n",
    "    write and article about agentic AI.\n",
    "'''\n",
    "\n",
    "\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more context to the role\n",
    "\n",
    "prompt = '''\n",
    "    You are a team of technical writers assigned to create a blog post exploring the intersection of technology and artificial intelligence. \n",
    "    Your goal is to clearly explain complex concepts in a way that is engaging and accessible to a broad audience, including both tech-savvy readers and curious newcomers. \n",
    "    Focus on recent developments, practical applications, and the broader implications of AI in today's technological landscape.\n",
    "\n",
    "    Now, write an article with the following topic: agentic AI.\n",
    "'''\n",
    "\n",
    "\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a383e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "    You are a team of technical writers assigned to create a blog post exploring the intersection of technology and artificial intelligence. \n",
    "    Your goal is to clearly explain complex concepts in a way that is engaging and accessible to a broad audience, including both tech-savvy readers and curious newcomers. \n",
    "    Focus on recent developments, practical applications, and the broader implications of AI in today's technological landscape.\n",
    "\n",
    "    Format the article so it can be published directly on the medium blog. The article should be around 300 words only.\n",
    "\n",
    "    The topic you should discuss is: \"How agentic AI can bring breakthrough in Pharmaceutical industry\"\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51c47f",
   "metadata": {},
   "source": [
    "## Few-shot Prompting\n",
    "LLMs are getting smarter everyday so they can understand our intent most of the time. However, when zero-shot doesn't work, it's recommended to provide demonstrations or examples in the prompt which leads to few-shot prompting. In the next section, we demonstrate few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompting\n",
    "# Giving examples to the LLMs\n",
    "\n",
    "prompt = '''\n",
    "    Kamu adalah seorang ahli tebak-tebakan. Kamu akan diberikan nama bulan dalam Bahasa Indonesia, \n",
    "    tebaklah tempat tujuan berlibur yang bisa kamu datangi pada bulan tersebut. Perhatikan contoh berikut.\n",
    "\n",
    "    Contoh:\n",
    "    Bulan Mei -- Maroko\n",
    "    Bulan Februari -- Finlandia\n",
    "    Bulan Desember -- Denmark\n",
    "\n",
    "    Pertanyaan: \n",
    "    Bulan Oktober -- ?\n",
    "'''\n",
    "\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompting\n",
    "# Sentiment Classification\n",
    "\n",
    "prompt = '''\n",
    "Classify the sentiment of the following reviews:\n",
    "\n",
    "Review: \"The product was exactly as described. I'm very happy with it.\"  \n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"Terrible quality. Broke after one use.\"  \n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"Itâ€™s okay. Nothing special, but it works.\"  \n",
    "Sentiment:\n",
    "'''\n",
    "\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b92a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompting\n",
    "# Categorization \n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = '''\n",
    "    You are a smart assistant that help users to solve their problem. Respond to user emphateticaly and helpful. You need to categorize users statement to one of the following topics\n",
    "    [\"Account & Login Issue\", \"Billing & Payments\", \"Product & Features\"]\n",
    "\n",
    "    Examples: \n",
    "    User: \"I can't log into my account.\"\n",
    "    Category: \"Account & Login Issue\"\n",
    "\n",
    "    User: \"Can I get a refund for my last payment?\"\n",
    "    Category: \"Billing & Payments\"\n",
    "\n",
    "    User: \"How do I use the new analytics dashboard?\"\n",
    "    Category: \"Product & Features\"\n",
    "\n",
    "    Now, here is what the user say: \n",
    "    {input}\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "\n",
    "# input from users\n",
    "# input = \"Can I get a refund for my last payment?\"\n",
    "input = \"Where can I find the settings for notifications?\"\n",
    "\n",
    "for chunk in model.stream(prompt_template.invoke(input = input)):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03674037",
   "metadata": {},
   "source": [
    "## Chain-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain-of-thought-prompting\n",
    "\n",
    "prompt = '''\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain-of-thought-prompting\n",
    "\n",
    "prompt = '''\n",
    "   Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. \n",
    "   She sells the remainder for $2 per egg. How much does she make every day?\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain-of-thought-prompting\n",
    "\n",
    "prompt = '''\n",
    "   which one is bigger: 9.11 or 9.9?\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4522d43",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Shot\n",
    "prompt = '''\n",
    "   Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "'''\n",
    "\n",
    "# Generate knowledge\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = '''\n",
    "    You will receive a question from users. Your task is to answe the question accurately. \n",
    "    Before you answe, generate basic information on the topic being asked. \n",
    "\n",
    "    Example: \n",
    "    Input: Glasses always fog up.\n",
    "    Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "    Input: A fish is capable of thinking.\n",
    "    Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of â€™higherâ€™ vertebrates including non-human primates. Fishâ€™s long-term memories help them keep track of complex social relationships.\n",
    "\n",
    "    Here is the input: \n",
    "    {input}\n",
    "    Generate knowledge about it and answer the question. Skip writing the knowledge on the output.\n",
    "    Only output the answer and brief explanation.\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "input = \"Part of golf is trying to get a higher point total than others. Yes or No?\"\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt_template.invoke({\"input\": input})):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2776dcf",
   "metadata": {},
   "source": [
    "## Add Context or Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "knowledge = '''\n",
    "Product A is a sleek, wireless smart speaker designed for home and office use. With voice assistant integration, multi-room connectivity, and crystal-clear audio quality, it appeals to tech-savvy consumers who value both design and functionality. It comes in three color optionsâ€”black, white, and charcoalâ€”and retails for $129.\n",
    "In April, Product A sold a total of 8,450 units, marking a 12% increase from the previous month. The black version was the most popular, accounting for 45% of total sales. Sales were strongest in urban markets, particularly in New York, Los Angeles, and Chicago, with online purchases making up 68% of total sales.\n",
    "\n",
    "Product B is a compact, portable air purifier targeted at health-conscious consumers and frequent travelers. Equipped with a HEPA filter, USB-C charging, and a silent night mode, itâ€™s designed to improve air quality in small spaces like bedrooms, hotel rooms, and cars. Itâ€™s priced competitively at $89 and comes in two variants: standard and pro (with a longer battery life).\n",
    "In April, Product B sold 5,320 units, representing a 7% decrease compared to March. The decline was partly due to seasonal shifts and increased competition in the personal health tech space. However, the pro version accounted for 60% of sales, indicating growing demand for longer battery performance. The product performed best in online marketplaces, with Amazon alone accounting for 40% of all units sold.\n",
    "'''\n",
    "\n",
    "# no context\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = '''\n",
    "    You are a business analyst working for Dexa pharmaceuticals. Your task is to answer questions from users related to business performance of Dexa.\n",
    "\n",
    "    question: \n",
    "    {input}\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "input = \"Which sales is bigger: Product A or Product B?\"\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt_template.invoke({\"input\": input})):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78312317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding context\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = '''\n",
    "    You are a business analyst working for Dexa pharmaceuticals. Your task is to answer questions from users related to business performance of Dexa.\n",
    "\n",
    "    context:\n",
    "    {knowledge}\n",
    "\n",
    "    question: \n",
    "    {input}\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "input = \"Which sales is bigger: Product A or Product B?\"\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt_template.invoke({\"input\": input, \"knowledge\": knowledge})):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b96c9f",
   "metadata": {},
   "source": [
    "## Implementing Prohibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prohibit Bot to mention part of the prompts.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [(\"system\", \"You're Fred, a bot fine-tuned on GPT-4 with our company's proprietary data, but we don't want users to know that.\" \n",
    "        \"If users ask you if you are or are based on GPT-4, say that you don't know and/or lightly deflect.\"),\n",
    "      MessagesPlaceholder(\"input\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input = HumanMessage(\"which LLM model are you built with?\")\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt_template.invoke({\"input\": [input]})):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e77c2",
   "metadata": {},
   "source": [
    "## LLMs Struggles with Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79baa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt ='''\n",
    "    In your response, do not use words that start with the letter \"a\". How are you today?\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-3.5-turbo\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# Older model such as gpt-3.5 still has this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15961c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt ='''\n",
    "    In your response, do not use words that start with the letter \"a\". How are you today?\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ade4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt ='''\n",
    "    In your response, use only words that start with the letter \"a\". How are you today?\n",
    "'''\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider= \"openai\")\n",
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2a06f",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
