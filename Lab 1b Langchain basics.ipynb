{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc7e592",
   "metadata": {},
   "source": [
    "# Lab 1b: Langchain Basics\n",
    "\n",
    "In this lab, you will learn about LangChain â€” an open-source framework that helps developers build powerful applications using large language models (LLMs) like OpenAI's GPT. You'll start by installing the necessary packages, then explore chat models and basic tool usage, which will serve as a foundation for building AI agents in later labs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194083b",
   "metadata": {},
   "source": [
    "## Installing LangChain\n",
    "We'll start by installing the langchain package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d87579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading environment variables \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  # take environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78251eab",
   "metadata": {},
   "source": [
    "The most basic package in the Langchain ecosystem is langchain-core, which contains all classes and abstractions required to build other packages, except LangSmith package.  \n",
    "<img src=\"https://python.langchain.com/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01205b3a",
   "metadata": {},
   "source": [
    "## Learning about Messages\n",
    "In Langchain, each message is defined by a role (e.g., \"user\", \"assistant\") and the content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider. LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
    "\n",
    "### Role \n",
    "Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n",
    "\n",
    "| Role | Description |\n",
    "|---|---|\n",
    "|system|Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.|\n",
    "|user|Represents input from a user interacting with the model, usually in the form of text or other interactive input.|\n",
    "|assistant|Represents a response from the model, which can include text or a request to invoke tools.|\n",
    "|tool|A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.|\n",
    "\n",
    "### Langchain Messages \n",
    "SystemMessage: corresponds to system role\n",
    "\n",
    "HumanMessage: corresponds to user role\n",
    "\n",
    "AIMessage: corresponds to assistant role\n",
    "\n",
    "AIMessageChunk: corresponds to assistant role, used for streaming responses\n",
    "\n",
    "ToolMessage: corresponds to tool role\n",
    "\n",
    "RemoveMessage -- does not correspond to any role. This is an abstraction, mostly used in LangGraph to manage chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720dbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, AIMessageChunk, RemoveMessage\n",
    "import json \n",
    "\n",
    "human_message = HumanMessage(\"Hello, I am a human.\")\n",
    "system_message = SystemMessage(\"Hello, I am a system message.\")\n",
    "ai_message = AIMessage(\"Hello, I am an AI.\")\n",
    "ai_message_chunk = AIMessageChunk(\"I am a chunk of AI Message.\")\n",
    "remove_message = RemoveMessage(id=\"123\")\n",
    "\n",
    "print(\"Human Message\")\n",
    "print(human_message.model_dump_json(indent=2))\n",
    "\n",
    "print(\"System Message\")\n",
    "print(system_message.model_dump_json(indent=2))\n",
    "\n",
    "print(\"AI Message\")\n",
    "print(ai_message.model_dump_json(indent=2))\n",
    "\n",
    "print(\"AI Message Chunk\")\n",
    "print(ai_message_chunk.model_dump_json(indent=2))\n",
    "\n",
    "print(\"Removed Message\")\n",
    "print(remove_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18248e88",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Prompt Template\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Berapa total penjualan produk {name}\")\n",
    "\n",
    "prompt_template.invoke({\"name\":\"A\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplates \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate( [\n",
    "    (\"system\", \"You are a database expert and your task is to write a SQL statement based on a question from user. \"\n",
    "        \"The SQL query statement shall be executed against an sqlite database.\"),\n",
    "    (\"user\", \"Berapa total penjualan produk {name}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"name\": \"A\"}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe48711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MessagesPlaceholder \n",
    "# Inserting the whole message instead of keywords.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [(\"system\", \"You are a database expert and your task is to write a SQL statement based on a question from user. \"\n",
    "        \"The SQL query statement shall be executed against an sqlite database.\"),\n",
    "      MessagesPlaceholder(\"msg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_template.invoke({\"msg\": [SystemMessage(content='The database contains MthlySales table, with the following columns: ID, PRODUCT_NAME, SALES_QTY, SALES_AMOUNT, MONTH'), \n",
    "                                HumanMessage(content=\"Berapa total penjualan produk A\")]}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [(\"system\", \"You are a database expert and your task is to write a SQL statement based on a question from user. \"\n",
    "        \"The SQL query statement shall be executed against an sqlite database.\"),\n",
    "      MessagesPlaceholder(\"msg\"),\n",
    "      (\"user\", \"Berapa jumlah produk {name} yang terjual di bulan {month}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_template.invoke({\"name\":\"A\", \n",
    "                        \"month\":\"3\", \n",
    "                        \"msg\": [SystemMessage(content='The database contains MthlySales table, with the following columns: ID, PRODUCT_NAME, SALES_QTY, SALES_AMOUNT, MONTH')]\n",
    "                        }).to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10563fa6",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "Chat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to using plain text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-mini\", model_provider= \"openai\")\n",
    "model.invoke(\"Please introduce yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee529cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to stream chat model responses\n",
    "\n",
    "for chunk in model.stream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5038be",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "Using the .with_structured_output() method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Class \n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ConceptList(BaseModel):\n",
    "    \"\"\"The list of concepts and brief description\"\"\"\n",
    "    concept: str = Field(description=\"Important concept to be explained.\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the concept\")\n",
    "\n",
    "structured_model = model.with_structured_output(ConceptList)\n",
    "\n",
    "structured_model.invoke(\"I am learning about generative AI. Explain 1 random concept related to it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51888aa5",
   "metadata": {},
   "source": [
    "If you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n",
    "\n",
    "Requirements\n",
    "\n",
    "> Core: langchain-core>=0.2.26\n",
    "\n",
    "> Typing extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypedDict \n",
    "\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class ConceptList(TypedDict):\n",
    "    \"\"\"The list of concepts and brief description\"\"\"\n",
    "    concept: Annotated[str,..., \"Important concept to be explained.\"] # type, default value, and description\n",
    "    explanation: Annotated[str, ... , \"Brief explanation of the concept\"]\n",
    "\n",
    "structured_model = model.with_structured_output(ConceptList)\n",
    "structured_model.invoke(\"I am learning about generative AI. Explain 1 random concept related to it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema\n",
    "# Equivalently, we can pass in a JSON Schema dict. \n",
    "# This requires no imports or classes and makes it very clear exactly how each parameter is documented, \n",
    "# at the cost of being a bit more verbose.\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"ConceptList\",\n",
    "    \"description\": \"The list of concepts and brief description\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"concept\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Important concept to be explained.\"\n",
    "        },\n",
    "        \"explanation\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Brief explanation of the concept.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"concept\", \"explanation\"]\n",
    "}\n",
    "\n",
    "structured_model = model.with_structured_output(json_schema)\n",
    "structured_model.invoke(\"I am learning about generative AI. Explain 1 random concept related to it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing Multiple Schema\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Union\n",
    "\n",
    "class ConceptList(BaseModel):\n",
    "    \"\"\"The list of concepts and brief description\"\"\"\n",
    "    concept: str = Field(description=\"Important concept to be explained.\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the concept\")\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    final_output: Union[ConceptList, ConversationalResponse]\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(FinalResponse)\n",
    "\n",
    "structured_model.invoke(\"I am learning about generative AI. Explain 1 random concept related to it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model.invoke(\"How do you feel about generative AI technology?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50306c",
   "metadata": {},
   "source": [
    "## Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Step 1: Set up the LLM\n",
    "llm = init_chat_model(\"gpt-4.1-mini\", model_provider= \"openai\", temperature = 0.7)\n",
    "llm.invoke(\"Please introduce yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 1: Summarize input text\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following article:\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b183221",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming industries by enabling machines to learn from data, \n",
    "make decisions, and even improve over time. Applications range from chatbots and virtual assistants \n",
    "to complex data analytics and autonomous vehicles. However, AI also brings challenges such as ethical concerns, \n",
    "bias in algorithms, and job displacement. As AI continues to evolve, balancing innovation with responsible \n",
    "development will be key to its long-term success.\n",
    "\"\"\"\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in summarize_chain.stream(input=input_text):\n",
    "    full_response += chunk.content\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 2: Generate 3 quiz questions from the summary\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"Based on the summary below, generate 3 quiz questions:\\n\\n{summary}\"\n",
    ")\n",
    "question_chain = question_prompt | llm\n",
    "\n",
    "for chunk in question_chain.stream(input=full_response):\n",
    "    full_response += chunk.content\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60542bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 3: Answer the first question\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"questions\"],\n",
    "    template=\"Pick the first question from below and answer it in detail:\\n\\n{questions}\"\n",
    ")\n",
    "answer_chain = answer_prompt | llm\n",
    "\n",
    "for chunk in answer_chain.stream(input=full_response):\n",
    "    full_response += chunk.content\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c174575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compose full chain\n",
    "full_chain = summarize_chain | question_chain | answer_chain\n",
    "\n",
    "for chunk in full_chain.stream(input=input_text):\n",
    "     print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b0b010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
